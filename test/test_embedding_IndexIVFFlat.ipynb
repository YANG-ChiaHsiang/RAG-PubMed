{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import faiss\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from multiprocessing import Pool, cpu_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"../data/pubmed_baseline/csv/pubmed25n1274.csv\" \n",
    "df = pl.read_csv(csv_path)\n",
    "print(f'Number of rows: {len(df)}')\n",
    "\n",
    "columns_to_check = [\"PMID\", \"Title\", \"Abstract\", \"Authors\", \"Year\", \"Journal\"]\n",
    "df = df.drop_nulls(subset=columns_to_check)\n",
    "print(f'Number of rows after dropping nulls: {len(df)}')\n",
    "\n",
    "df = df.with_columns(df[\"Year\"].cast(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.parallel import DistributedDataParallel as DDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# è¼‰å…¥ MedEmbed æ¨¡å‹\n",
    "model_name = \"abhinand/MedEmbed-base-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "model = model.half()\n",
    "\n",
    "model = DDP(model, device_ids=[0,1,3,4], output_device=0, find_unused_parameters=True)\n",
    "# if torch.cuda.device_count() > 1:\n",
    "#     model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = df[\"Title\"] + \". \" + df[\"Abstract\"]\n",
    "\n",
    "def embed_text_in_batches(texts, batch_size=32):\n",
    "    texts = list(texts)  # **ç¢ºä¿ texts æ˜¯ List**\n",
    "    all_embeddings = []\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Processing Batches\"):\n",
    "        batch_texts = texts[i:i+batch_size]  # ç›´æ¥å– list çš„ sliceï¼Œä¸éœ€è¦ `.tolist()`\n",
    "        inputs = tokenizer(batch_texts, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        batch_embeddings = outputs.last_hidden_state[:, 0, :].to(torch.float32).cpu().numpy()  # å– CLS token\n",
    "        all_embeddings.append(batch_embeddings)\n",
    "\n",
    "        del inputs, outputs  # é‡‹æ”¾è¨˜æ†¶é«”\n",
    "        torch.cuda.empty_cache()  # æ¸…ç† GPU è¨˜æ†¶é«”\n",
    "\n",
    "    return np.vstack(all_embeddings)  # åˆä½µæ‰€æœ‰ batch çµæœ\n",
    "\n",
    "# ç”¢ç”Ÿæ‰€æœ‰æ–‡æœ¬çš„å‘é‡\n",
    "embeddings = embed_text_in_batches(texts, batch_size=9000)  # **å¦‚æœé‚„æ˜¯ OutOfMemoryï¼Œæ”¹ batch_size=8**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è¨­ç½® FAISS å‘é‡åº«\n",
    "d = embeddings.shape[1]  # å‘é‡ç¶­åº¦\n",
    "N = embeddings.shape[0] # number of embeddings\n",
    "nlist = nlist = min(int(4 * np.sqrt(N)), N)\n",
    "quantizer = faiss.IndexFlatL2(d)  # L2 è·é›¢çš„é‡åŒ–å™¨\n",
    "index = faiss.IndexIVFFlat(quantizer, d, nlist, faiss.METRIC_L2)\n",
    "\n",
    "# è¨“ç·´ FAISS ç´¢å¼•ï¼ˆIVF éœ€è¦è¨“ç·´ï¼‰\n",
    "index.train(embeddings)\n",
    "index.add(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å„²å­˜ FAISS ç´¢å¼•\n",
    "faiss.write_index(index, \"faiss_medical_index_IndexIVFFlat.ivf\")\n",
    "\n",
    "df.write_csv(\"faiss_metadata_IndexIVFFlat.csv\")\n",
    "\n",
    "print(\"å‘é‡ç´¢å¼•å»ºç«‹å®Œæˆï¼Œå·²å„²å­˜ FAISS ç´¢å¼•å’Œ Metadataï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Serach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis900412/anaconda3/envs/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# è¨­å®šè¨­å‚™\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# é‡æ–°è¼‰å…¥æ¨¡å‹ & Tokenizer\n",
    "model_name = \"abhinand/MedEmbed-base-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device).eval()\n",
    "\n",
    "# é‡æ–°è¼‰å…¥ FAISS ç´¢å¼•\n",
    "index = faiss.read_index(\"../output/2020/faiss.index\")\n",
    "\n",
    "# è®€å– Metadata\n",
    "df = pd.read_csv(\"../output/2020/metadata.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¹ 1. Self-supervised learning for medical image analysis: Discriminative, restorative, or adversarial? (2024)\n",
      "    ğŸ“ Abstract: Discriminative, restorative, and adversarial learning have proven beneficial for self-supervised learning schemes in computer vision and medical imaging. Existing efforts, however, fail to capitalize on the potentially synergistic effects these methods may offer in a ternary setup, which, we envision can significantly benefit deep semantic representation learning. Towards this end, we developed DiRA, the first framework that unites discriminative, restorative, and adversarial learning in a unified manner to collaboratively glean complementary visual information from unlabeled medical images for fine-grained semantic representation learning. Our extensive experiments demonstrate that DiRA: (1) encourages collaborative learning among three learning ingredients, resulting in more generalizable representation across organs, diseases, and modalities; (2) outperforms fully supervised ImageNet models and increases robustness in small data regimes, reducing annotation cost across multiple medical imaging applications; (3) learns fine-grained semantic representation, facilitating accurate lesion localization with only image-level annotation; (4) improves reusability of low/mid-level features; and (5) enhances restorative self-supervised approaches, revealing that DiRA is a general framework for united representation learning. Code and pretrained models are available at https://github.com/JLiangLab/DiRA.\n",
      "    ğŸ‘©â€âš•ï¸ Authors: Fatemeh Haghighi, Mohammad Reza Hosseinzadeh Taher, Michael B Gotway, Jianming Liang\n",
      "    ğŸ¥ Journal: Medical image analysis\n",
      "    ğŸ”‘ Keywords: Fine-grained representation learning&Self-supervised Learning&Transfer Learning\n",
      "    ğŸ”— PMID: 38537414\n",
      "\n",
      "ğŸ”¹ 2. Weakly-Supervised 3D Medical Image Segmentation Using Geometric Prior and Contrastive Similarity. (2023)\n",
      "    ğŸ“ Abstract: Medical image segmentation is almost the most important pre-processing procedure in computer-aided diagnosis but is also a very challenging task due to the complex shapes of segments and various artifacts caused by medical imaging, (i.e., low-contrast tissues, and non-homogenous textures). In this paper, we propose a simple yet effective segmentation framework that incorporates the geometric prior and contrastive similarity into the weakly-supervised segmentation framework in a loss-based fashion. The proposed geometric prior built on point cloud provides meticulous geometry to the weakly-supervised segmentation proposal, which serves as better supervision than the inherent property of the bounding-box annotation (i.e., height and width). Furthermore, we propose the contrastive similarity to encourage organ pixels to gather around in the contrastive embedding space, which helps better distinguish low-contrast tissues. The proposed contrastive embedding space can make up for the poor representation of the conventionally-used gray space. Extensive experiments are conducted to verify the effectiveness and the robustness of the proposed weakly-supervised segmentation framework. The proposed framework are superior to state-of-the-art weakly-supervised methods on the following publicly accessible datasets: LiTS 2017 Challenge, KiTS 2021 Challenge and LPBA40. We also dissect our method and evaluate the performance of each component.\n",
      "    ğŸ‘©â€âš•ï¸ Authors: Hao Du, Qihua Dong, Yan Xu, Jing Liao\n",
      "    ğŸ¥ Journal: IEEE transactions on medical imaging\n",
      "    ğŸ”‘ Keywords: nan\n",
      "    ğŸ”— PMID: 37093730\n",
      "\n",
      "ğŸ”¹ 3. Semantic-Aware Contrastive Learning for Multi-Object Medical Image Segmentation. (2023)\n",
      "    ğŸ“ Abstract: Medical image segmentation, or computing voxel-wise semantic masks, is a fundamental yet challenging task in medical imaging domain. To increase the ability of encoder-decoder neural networks to perform this task across large clinical cohorts, contrastive learning provides an opportunity to stabilize model initialization and enhances downstream tasks performance without ground-truth voxel-wise labels. However, multiple target objects with different semantic meanings and contrast level may exist in a single image, which poses a problem for adapting traditional contrastive learning methods from prevalent \"image-level classification\" to \"pixel-level segmentation\". In this article, we propose a simple semantic-aware contrastive learning approach leveraging attention masks and image-wise labels to advance multi-object semantic segmentation. Briefly, we embed different semantic objects to different clusters rather than the traditional image-level embeddings. We evaluate our proposed method on a multi-organ medical image segmentation task with both in-house data and MICCAI Challenge 2015 BTCV datasets. Compared with current state-of-the-art training strategies, our proposed pipeline yields a substantial improvement of 5.53% and 6.09% on Dice score for both medical image segmentation cohorts respectively (p-value 0.01). The performance of the proposed method is further assessed on external medical image cohort via MICCAI Challenge FLARE 2021 dataset, and achieves a substantial improvement from Dice 0.922 to 0.933 (p-value 0.01).\n",
      "    ğŸ‘©â€âš•ï¸ Authors: Ho Hin Lee, Yucheng Tang, Qi Yang, Xin Yu, Leon Y Cai, Lucas W Remedios, Shunxing Bao, Bennett A Landman, Yuankai Huo\n",
      "    ğŸ¥ Journal: IEEE journal of biomedical and health informatics\n",
      "    ğŸ”‘ Keywords: nan\n",
      "    ğŸ”— PMID: 37310834\n",
      "\n",
      "ğŸ”¹ 4. Unsupervised Representation Learning for Tissue Segmentation in Histopathological Images: From Global to Local Contrast. (2022)\n",
      "    ğŸ“ Abstract: Tissue segmentation is an essential task in computational pathology. However, relevant datasets for such a pixel-level classification task are hard to obtain due to the difficulty of annotation, bringing obstacles for training a deep learning-based segmentation model. Recently, contrastive learning has provided a feasible solution for mitigating the heavy reliance of deep learning models on annotation. Nevertheless, applying contrastive loss to the most abstract image representations, existing contrastive learning frameworks focus on global features, therefore, are less capable of encoding finer-grained features (e.g., pixel-level discrimination) for the tissue segmentation task. Enlightened by domain knowledge, we design three contrastive learning tasks with multi-granularity views (from global to local) for encoding necessary features into representations without accessing annotations. Specifically, we construct: (1) an image-level task to capture the difference between tissue components, i.e., encoding the component discrimination; (2) a superpixel-level task to learn discriminative representations of local regions with different tissue components, i.e., encoding the prototype discrimination; (3) a pixel-level task to encourage similar representations of different tissue components within a local region, i.e., encoding the spatial smoothness. Through our global-to-local pre-training strategy, the learned representations can reasonably capture the domain-specific and fine-grained patterns, making them easily transferable to various tissue segmentation tasks in histopathological images. We conduct extensive experiments on two tissue segmentation datasets, while considering two real-world scenarios with limited or sparse annotations. The experimental results demonstrate that our framework is superior to existing contrastive learning methods and can be easily combined with weakly supervised and semi-supervised segmentation methods.\n",
      "    ğŸ‘©â€âš•ï¸ Authors: Zeyu Gao, Chang Jia, Yang Li, Xianli Zhang, Bangyang Hong, Jialun Wu, Tieliang Gong, Chunbao Wang, Deyu Meng, Yefeng Zheng, Chen Li\n",
      "    ğŸ¥ Journal: IEEE transactions on medical imaging\n",
      "    ğŸ”‘ Keywords: nan\n",
      "    ğŸ”— PMID: 35839184\n",
      "\n",
      "ğŸ”¹ 5. Distributed contrastive learning for medical image segmentation. (2022)\n",
      "    ğŸ“ Abstract: Supervised deep learning needs a large amount of labeled data to achieve high performance. However, in medical imaging analysis, each site may only have a limited amount of data and labels, which makes learning ineffective. Federated learning (FL) can learn a shared model from decentralized data. But traditional FL requires fully-labeled data for training, which is very expensive to obtain. Self-supervised contrastive learning (CL) can learn from unlabeled data for pre-training, followed by fine-tuning with limited annotations. However, when adopting CL in FL, the limited data diversity on each site makes federated contrastive learning (FCL) ineffective. In this work, we propose two federated self-supervised learning frameworks for volumetric medical image segmentation with limited annotations. The first one features high accuracy and fits high-performance servers with high-speed connections. The second one features lower communication costs, suitable for mobile devices. In the first framework, features are exchanged during FCL to provide diverse contrastive data to each site for effective local CL while keeping raw data private. Global structural matching aligns local and remote features for a unified feature space among different sites. In the second framework, to reduce the communication cost for feature exchanging, we propose an optimized method FCLOpt that does not rely on negative samples. To reduce the communications of model download, we propose the predictive target network update (PTNU) that predicts the parameters of the target network. Based on PTNU, we propose the distance prediction (DP) to remove most of the uploads of the target network. Experiments on a cardiac MRI dataset show the proposed two frameworks substantially improve the segmentation and generalization performance compared with state-of-the-art techniques.\n",
      "    ğŸ‘©â€âš•ï¸ Authors: Yawen Wu, Dewen Zeng, Zhepeng Wang, Yiyu Shi, Jingtong Hu\n",
      "    ğŸ¥ Journal: Medical image analysis\n",
      "    ğŸ”‘ Keywords: Contrastive learning&Federated learning&Image segmentation&Self-supervised learning\n",
      "    ğŸ”— PMID: 35994968\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# **æœå°‹å‡½æ•¸**\n",
    "def search_papers(query, top_k=5):\n",
    "    # **Step 1: Query å‘é‡åŒ–**\n",
    "    inputs = tokenizer(query, padding=True, truncation=True, return_tensors=\"pt\", max_length=512).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    query_embedding = outputs.last_hidden_state[:, 0, :].to(torch.float32).cpu().numpy()\n",
    "\n",
    "    # **Step 2: FAISS æª¢ç´¢**\n",
    "    D, I = index.search(query_embedding, top_k)  # D æ˜¯è·é›¢ï¼ŒI æ˜¯ç´¢å¼•\n",
    "\n",
    "    # **Step 3: æ ¹æ“šç´¢å¼•å–å¾— Metadata**\n",
    "    results = df.iloc[I[0]].copy()  # é¸æ“‡æŸ¥è©¢åˆ°çš„è«–æ–‡\n",
    "\n",
    "    # **Step 4: æŒ‰å¹´ä»½æ’åºï¼ˆå¾æ–°åˆ°èˆŠï¼‰**\n",
    "    results = results.sort_values(by=\"Year\", ascending=False)\n",
    "\n",
    "    # **Step 5: æ ¼å¼åŒ–è¼¸å‡º**\n",
    "    search_results = []\n",
    "    for _, row in results.iterrows():\n",
    "        search_results.append({\n",
    "            \"PMID\": row[\"PMID\"],\n",
    "            \"Title\": row[\"Title\"],\n",
    "            \"Abstract\": row[\"Abstract\"],\n",
    "            \"Authors\": row[\"Authors\"],\n",
    "            \"Year\": row[\"Year\"],\n",
    "            \"Journal\": row[\"Journal\"],\n",
    "            \"Keyword\": row[\"Keyword\"]\n",
    "        })\n",
    "\n",
    "    return search_results\n",
    "\n",
    "# **æ¸¬è©¦ Search**\n",
    "query = \"Contrastive learning for medical image analysis\"\n",
    "results = search_papers(query, top_k=5)\n",
    "\n",
    "# **é¡¯ç¤ºçµæœ**\n",
    "for i, res in enumerate(results):\n",
    "    print(f\"ğŸ”¹ {i+1}. {res['Title']} ({res['Year']})\")\n",
    "    print(f\"    ğŸ“ Abstract: {res['Abstract']}\")\n",
    "    print(f\"    ğŸ‘©â€âš•ï¸ Authors: {res['Authors']}\")\n",
    "    print(f\"    ğŸ¥ Journal: {res['Journal']}\")\n",
    "    print(f\"    ğŸ”‘ Keywords: {res['Keyword']}\")\n",
    "    print(f\"    ğŸ”— PMID: {res['PMID']}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
