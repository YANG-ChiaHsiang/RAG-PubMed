{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/louis900412/anaconda3/envs/rag/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 選擇設備\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "\n",
    "# 載入 Tokenizer & Model\n",
    "# model_name = \"jinaai/jina-embeddings-v3\"\n",
    "model_name = \"abhinand/MedEmbed-base-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正：將 BFloat16 轉換為 Float32\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).last_hidden_state  # 所有 token 的輸出\n",
    "    embedding = outputs.mean(dim=1)  # 均值池化\n",
    "    return embedding.to(torch.float32).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Probabilistic Attention Map: A Probabilistic Attention Mechanism for Convolutional Neural Networks.\n",
      "Abstract: The attention mechanism is essential to \n",
      "\n",
      "Title: Time-Series Representation Feature Refinement with a Learnable Masking Augmentation Framework in Contrastive Learning.\n",
      "Abstract: In this study, we propose a novel framework for time-series representation learning that integrates a learnable masking-augmentation strategy into a contrastive learning framework. Time-series data pose challenges due to their temporal dependencies and feature-extraction complexities. To address these challenges, we introduce a masking-based reconstruction approach within a contrastive learning context, aiming to enhance the model's ability to learn discriminative temporal features. Our method leverages self-supervised learning to effectively capture both global and local patterns by strategically masking segments of the time-series data and reconstructing them, which aids in revealing nuanced temporal dependencies. We utilize learnable masking as a dynamic augmentation technique, which enables the model to optimize contextual relationships in the data and extract meaningful representations that are both context-aware and robust. Extensive experiments were conducted on multiple time-series datasets, including SleepEDF-78, 20, UCI-HAR, achieving improvements of 2%, 2.55%, and 3.89% each and similar performance on Epilepsy in accuracy over baseline methods. Our results show significant performance gains compared to existing methods, highlighting the potential of our framework to advance the field of time-series analysis by improving the quality of learned representations and enhancing downstream task performance.\n",
      "\n",
      "Title: Automated Screening of Precancerous Cervical Cells Through Contrastive Self-Supervised Learning.\n",
      "Abstract: Cervical cancer is a significant health challenge, yet it can be effectively prevented through early detection. Cytology-based screening is critical for identifying cancerous and precancerous lesions; however, the process is labor-intensive and reliant on trained experts to scan through hundreds of thousands of mostly normal cells. To address these challenges, we propose a novel distribution-augmented approach using contrastive self-supervised learning for detecting abnormal squamous cervical cells from cytological images. Our method utilizes color augmentations to enhance the model's ability to differentiate between normal and high-grade precancerous cells; specifically, high-grade squamous intraepithelial lesions (HSILs) and atypical squamous cells-cannot exclude HSIL (ASC-H). Our model was trained exclusively on normal cervical cell images and achieved high diagnostic accuracy, demonstrating robustness against color distribution shifts. We employed kernel density estimation (KDE) to assess cell type distributions, further facilitating the identification of abnormalities. Our results indicate that our approach improves screening accuracy and reduces the workload for cytopathologists, contributing to more efficient cervical cancer screening programs.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def search_papers(query, top_k=5, threshold=0.5):\n",
    "    index = faiss.read_index(\"pubmed_index.faiss\")\n",
    "    metadata = np.load(\"pubmed_metadata.npy\", allow_pickle=True)\n",
    "\n",
    "    query_emb = np.array(generate_embedding(query)).reshape(1, -1).astype(\"float32\")\n",
    "\n",
    "    # 執行檢索\n",
    "    D, I = index.search(query_emb, top_k)\n",
    "\n",
    "    # 過濾掉相似度低於 threshold 的結果\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        if idx < len(metadata) and D[0][i] >= threshold:\n",
    "            paper = metadata[idx]\n",
    "            if paper and isinstance(paper.get(\"Title\"), str) and isinstance(paper.get(\"Abstract\"), str):\n",
    "                results.append(paper)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 測試查詢\n",
    "query = \"Image & Contrastive Learning\"\n",
    "search_results = search_papers(query, top_k=3)\n",
    "for paper in search_results:\n",
    "    print(f\"Title: {paper.get('Title', 'N/A')}\\nAbstract: {paper.get('Abstract', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_retrieved_docs(search_results):\n",
    "    context = \"\"\n",
    "    for i, doc in enumerate(search_results):\n",
    "        title = doc.get(\"Title\", \"Unknown Title\")  # 確保標題存在\n",
    "        abstract = doc.get(\"Abstract\", \"No abstract available.\")  # 確保摘要存在\n",
    "        context += f\"Document {i+1}:\\nTitle: {title}\\nAbstract: {abstract}\\n\\n\"\n",
    "    return context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_rag_prompt(query, search_results):\n",
    "    context = format_retrieved_docs(search_results)\n",
    "    \n",
    "    prompt = f\"\"\"You are a medical expert specializing in kidney ultrasound imaging.\n",
    "    \n",
    "    The user has asked the following question:\n",
    "    \"{query}\"\n",
    "    \n",
    "    Based on the retrieved relevant documents below, generate a well-informed answer.\n",
    "    \n",
    "    Retrieved Documents:\n",
    "    {context}\n",
    "    \n",
    "    Answer the user's query using the most relevant information. If necessary, provide additional insights based on your knowledge.\n",
    "    \n",
    "    Response:\n",
    "    \"\"\"\n",
    "    \n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document 1:\n",
      "Title: Probabilistic Attention Map: A Probabilistic Attention Mechanism for Convolutional Neural Networks. (2024.0)\n",
      "Document 2:\n",
      "Title: Time-Series Representation Feature Refinement with a Learnable Masking Augmentation Framework in Contrastive Learning. (2024.0)\n",
      "Document 3:\n",
      "Title: Automated Screening of Precancerous Cervical Cells Through Contrastive Self-Supervised Learning. (2024.0)\n",
      "\n",
      "===============================\n",
      "\n",
      "\"Image & Contrastive Learning\" refers to a method used in machine learning, specifically in the context of self-supervised learning, to improve the extraction of meaningful representations from images. This approach plays a significant role in healthcare, particularly in tasks like time-series analysis and cervical cancer screening, as suggested by the documents.\n",
      "\n",
      "For instance, Document 2 introduces a masking-augmentation strategy incorporated into a contrastive learning framework for time-series representation learning. This approach allows for the optimization of contextual relationships in the data and extraction of context-aware and robust representations. It has shown improvements in accuracy over baseline methods in several time-series datasets.\n",
      "\n",
      "Similarly, in Document 3, contrastive self-supervised learning is used for detecting abnormal squamous cervical cells from cytological images. The model utilizes color augmentations to enhance its ability to differentiate between normal and high-grade precancerous cells. The model was trained exclusively on normal cervical cell images and achieved high diagnostic accuracy, demonstrating robustness against color distribution shifts.\n",
      "\n",
      "In the context of kidney ultrasound analysis, these techniques could potentially be applied to improve the detection and characterization of kidney lesions, or to distinguish between different stages of kidney diseases. This could lead to more accurate diagnoses and better patient outcomes. However, this would require further research and validation to ensure the effectiveness and reliability of such approaches in the specific context of kidney ultrasound imaging.\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "\n",
    "API_KEY = \"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\"\n",
    "\n",
    "client = OpenAI(api_key=API_KEY)  # 用 OpenAI 客戶端初始化\n",
    "\n",
    "search_results = search_papers(query, top_k=3)\n",
    "# 檢查檢索結果是否完整\n",
    "for i, paper in enumerate(search_results):\n",
    "    print(f\"Document {i+1}:\")\n",
    "    print(f\"Title: {paper.get('Title', 'MISSING TITLE')} ({paper.get('Year', 'N/A')})\")\n",
    "    # print(f\"Abstract: {paper.get('Abstract', 'MISSING ABSTRACT')}\\n\")\n",
    "\n",
    "print(\"\\n===============================\\n\")\n",
    "prompt = generate_rag_prompt(query, search_results)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"gpt-4\",\n",
    "    messages=[\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert in kidney ultrasound analysis.\"},\n",
    "        {\"role\": \"user\", \"content\": prompt}\n",
    "    ],\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "generated_answer = response.choices[0].message.content\n",
    "print(generated_answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
