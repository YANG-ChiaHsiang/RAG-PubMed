{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "csv_path = \"../data/pubmed_baseline/csv/pubmed25n0001.csv\" \n",
    "# csv_path = \"../data/pubmed_baseline/merged_output.csv\" \n",
    "\n",
    "df = pl.read_csv(csv_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "\n",
    "# 選擇設備\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel\n",
    "import os\n",
    "\n",
    "model_name = \"abhinand/MedEmbed-base-v0.1\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
    "# model = AutoModel.from_pretrained(model_name, trust_remote_code=True).to(device)\n",
    "model = AutoModel.from_pretrained(model_name, trust_remote_code=True).half().to(device)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=[0, 1])\n",
    "\n",
    "model.eval()\n",
    "BATCH_SIZE = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(texts):\n",
    "    inputs = tokenizer(texts, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).last_hidden_state\n",
    "    embeddings = outputs.mean(dim=1)\n",
    "    return embeddings.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "# 設定 embedding 維度（根據 jinaai/jina-embeddings-v3，應該是 1024 維）\n",
    "embedding_dim = 768  \n",
    "index = faiss.IndexFlatIP(embedding_dim)  # 使用 L2 距離（內積 IndexFlatIP 也可以）\n",
    "\n",
    "paper_metadata = []\n",
    "all_embeddings = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# 準備數據\n",
    "texts = []\n",
    "for row in df.iter_rows(named=True):\n",
    "    text = f\"{row['Title']} {row['Abstract']}\"\n",
    "    texts.append(text)\n",
    "    paper_metadata.append({\n",
    "        \"PMID\": row[\"PMID\"], \n",
    "        \"Title\": row[\"Title\"], \n",
    "        \"Abstract\": row[\"Abstract\"], \n",
    "        \"Year\": row[\"Year\"]\n",
    "    })\n",
    "\n",
    "# 批次處理\n",
    "for i in tqdm(range(0, len(texts), BATCH_SIZE)):\n",
    "    batch_texts = texts[i:i + BATCH_SIZE]\n",
    "    batch_embeddings = generate_embeddings_batch(batch_texts)\n",
    "    all_embeddings.extend(batch_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 轉換為 numpy array 並加入 FAISS\n",
    "all_embeddings = np.array(all_embeddings).astype(\"float32\")\n",
    "index.add(all_embeddings)\n",
    "\n",
    "# 儲存\n",
    "faiss.write_index(index, \"pubmed_index.faiss\")\n",
    "np.save(\"pubmed_metadata.npy\", paper_metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修正：將 BFloat16 轉換為 Float32\n",
    "def generate_embedding(text):\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs).last_hidden_state  # 所有 token 的輸出\n",
    "    embedding = outputs.mean(dim=1)  # 均值池化\n",
    "    return embedding.to(torch.float32).cpu().numpy().flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def search_papers(query, top_k=5, threshold=0.5):\n",
    "    index = faiss.read_index(\"pubmed_index.faiss\")\n",
    "    metadata = np.load(\"pubmed_metadata.npy\", allow_pickle=True)\n",
    "\n",
    "    query_emb = np.array(generate_embedding(query)).reshape(1, -1).astype(\"float32\")\n",
    "\n",
    "    # 執行檢索\n",
    "    D, I = index.search(query_emb, top_k)\n",
    "\n",
    "    # 過濾掉相似度低於 threshold 的結果\n",
    "    results = []\n",
    "    for i, idx in enumerate(I[0]):\n",
    "        if idx < len(metadata) and D[0][i] >= threshold:\n",
    "            paper = metadata[idx]\n",
    "            if paper and isinstance(paper.get(\"Title\"), str) and isinstance(paper.get(\"Abstract\"), str):\n",
    "                results.append(paper)\n",
    "\n",
    "    return results\n",
    "\n",
    "# 測試查詢\n",
    "query = \"mdeical\"\n",
    "search_results = search_papers(query, top_k=3)\n",
    "for paper in search_results:\n",
    "    print(f\"Title: {paper.get('Title', 'N/A')}\\nAbstract: {paper.get('Abstract', 'N/A')}\\nYear: {paper.get('Year', 'N/A')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
